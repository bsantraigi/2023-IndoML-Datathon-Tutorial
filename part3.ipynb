{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndoML 2023 Tuturial: Part 2\n",
    "## The Era of LLMs!\n",
    "\n",
    "### In-context learning and Prompt Engineering\n",
    "\n",
    "\n",
    "1. We will use recent LLMs like GPT-3/FLAN-T5/LLAMA to query the models in natural language to get answers/predictions.\n",
    "2. These models are finetuned on instructions or human-feedbacks to enable them to perform a task through \"prompting\".\n",
    "3. Best part is we wouldn't need to train our models to get started, direct inference from these pretrained models is fine.\n",
    "    * NOTE: Although there can be methods to finetune these models on our data to get better results, we will not be covering that in this tutorial.\n",
    "\n",
    "### Methods that we will try:\n",
    "\n",
    "1. FLAN-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlsasfs/home/ttbhashini/arroy/anaconda3/envs/py38-new/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "dataset = load_dataset(\"AmazonScience/massive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `AutoTokenizer` and `AutoModelForSeq2SeqLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers accelerate bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# checkpoint = \"bigscience/mt0-base\"\n",
    "checkpoint = \"bigscience/bloomz-3b\"\n",
    "# checkpoint = \"google/flan-t5-xxl\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specific Example of Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"Detect the intent class of the utterance.\\nUtterance: I am going to school.; Intent:\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- We will create prompts for each test sample in the dataset. \n",
    "- There are few ways to format these prompt and this step is called \"Prompt Engineering\".\n",
    "    - Few-shot In-context learning: Use task-description and examples\n",
    "    - Zero-shot In-context learning: Use task-description only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplars for the multilingual intent-detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather examples from the training dataset\n",
    "import pandas as pd\n",
    "df_train = dataset['train'].to_pandas()\n",
    "\n",
    "# Extract one random sample per intent, we will randomly sample the rows\n",
    "# Hope is that the model will predict english labels for any language that way.\n",
    "df_intent_samples = df_train.groupby(\"intent\").apply(lambda x: x.sample(1, random_state=42)).reset_index(drop=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add formatted prompt for each sample\n",
    "def int2str(x):\n",
    "    return dataset['train'].features['intent'].int2str(x)\n",
    "\n",
    "df_intent_samples['example_prompt_format'] = df_intent_samples.apply(lambda x: f'Utterance: {x[\"utt\"]}; Intent: {int2str(x[\"intent\"])}', axis=1)\n",
    "\n",
    "# merge examples into a single string\n",
    "prompt_exemplars = df_intent_samples['example_prompt_format'].str.cat(sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_exemplars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prompts for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new feature column to the dataset\n",
    "# Prompt: What is the intent of the following sentence?\\m \"{utt}\"\n",
    "\n",
    "few_shot = True\n",
    "def add_prompt(example):\n",
    "    if few_shot:\n",
    "        example[\"prompt\"] = f'# Detect intent of the input utterance.\\n\\n{prompt_exemplars}\\nUtterance: {example[\"utt\"]}; Intent:'\n",
    "    else:\n",
    "        example[\"prompt\"] = f'# Detect intent of the input utterance.\\n\\nUtterance: {example[\"utt\"]}; Intent:'\n",
    "    \n",
    "    example[\"str_label\"] = int2str(example[\"intent\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "extended_eval_set = dataset['validation'].map(add_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_eval_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try to predict using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extended_eval_set[100]['prompt'])\n",
    "\n",
    "x = extended_eval_set[100]['prompt']\n",
    "tok_x = tokenizer(x, return_tensors=\"pt\")\n",
    "y = model.generate(tok_x['input_ids'].to(\"cuda\"), num_beams=5, num_return_sequences=5, max_length=2000)\n",
    "output = tokenizer.decode(y[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_class = dataset['train'].features['intent'].num_classes\n",
    "str2int = {}\n",
    "for i in range(N_class):\n",
    "    str2int[int2str(i)] = i\n",
    "\n",
    "def parse_prediction(prompt, output_txt):\n",
    "    # take the diff between the prompt and the generated text\n",
    "    # cut it till the first \\n\n",
    "    pred_class = output_txt[len(prompt):].split('\\n')[0].strip()\n",
    "\n",
    "    # Check if it matches any label in the dataset\n",
    "    if pred_class in str2int:\n",
    "        return str2int[pred_class], pred_class\n",
    "    else:\n",
    "        return -1, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(str2int.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_prediction(x, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# We need to define a compute_metric function that is supported by the Trainer output\n",
    "# It basically converts the logits to predictions and then calls the metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # print(\"Predictions: \", predictions)\n",
    "    # print(\"Labels: \", labels)\n",
    "    return metric_f1.compute(predictions=predictions, references=labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a smaller subset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Select random samples from above subsets\n",
    "n_eval = 600\n",
    "eval_indices = random.sample(range(len(extended_eval_set)), n_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_subset = extended_eval_set.select(eval_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather prediction for the eval set\n",
    "outputs = []\n",
    "for i in tqdm(range(len(eval_data_subset))):\n",
    "    x = eval_data_subset[i]['prompt']\n",
    "    tok_x = tokenizer(x, return_tensors=\"pt\")\n",
    "    y = model.generate(tok_x['input_ids'].to(\"cuda\"), num_beams=3, num_return_sequences=3, max_length=2000)\n",
    "    all_predictions = []\n",
    "    for j in range(len(y)):\n",
    "        output = tokenizer.decode(y[j], skip_special_tokens=True)\n",
    "        all_predictions.append(output)\n",
    "    outputs.append(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the predictions\n",
    "parsed_predictions = []\n",
    "for i in tqdm(range(len(eval_data_subset))):\n",
    "    x = eval_data_subset[i]['prompt']\n",
    "    all_predictions = outputs[i]\n",
    "    parsed_predictions.append([parse_prediction(x, output) for output in all_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = eval_data_subset.to_pandas()\n",
    "df_eval['pred_label_str'] = [x[1] for x in parsed_predictions]\n",
    "df_eval['pred_label_int'] = [x[0] for x in parsed_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute f1 for each locale and plot f1 scores vs locales\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11, 8)}) # Setting some matplotlib configs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print histogram of locale, y-axis normalized to 1\n",
    "P = sns.histplot(df_eval['locale'], stat=\"probability\")\n",
    "\n",
    "P.set_xlabel(\"Locale\")\n",
    "P.set_ylabel(\"Probability\")\n",
    "P.set_title(\"Locale histogram\")\n",
    "\n",
    "# Rotate x labels by 90 degrees\n",
    "for item in P.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(df):\n",
    "    return f1_score(df['targets'], df['predictions'], average='macro')\n",
    "\n",
    "def compute_acc(df):\n",
    "    return accuracy_score(df['targets'], df['predictions'])\n",
    "\n",
    "df_f1 = df_eval.groupby('locale').apply(compute_f1)\n",
    "P = sns.barplot(x=df_f1.index, y=df_f1.values)\n",
    "# df_acc = df.groupby('locale').apply(compute_acc)\n",
    "# P = sns.barplot(x=df_acc.index, y=df_acc.values)\n",
    "\n",
    "P.set_title(\"Locale vs Performance\")\n",
    "P.set_xlabel(\"Locale\")\n",
    "P.set_ylabel(\"Performance\")\n",
    "\n",
    "# Rotate x labels by 90 degrees\n",
    "for item in P.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38-new)",
   "language": "python",
   "name": "py38-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
